{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "\n",
    "print(\"Characters:\", total_characters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    \n",
    "    \"context_length\": 256,  \n",
    "    \"embedding_dim\": 768,    \n",
    "    \"n_heads\": 12,          \n",
    "    \"n_layers\": 12,         \n",
    "    \"dropout\": 0.1,         \n",
    "}\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, num_heads, context_length):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_querys = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_projection = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        # Causal mask for autoregressive processing\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch, num_tokens, dim = inputs.shape\n",
    "        query = self.W_querys(inputs)\n",
    "        key = self.W_keys(inputs)\n",
    "        value = self.W_values(inputs)\n",
    "\n",
    "        query = query.view(batch, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(batch, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        mask_bool = self.mask[:num_tokens, :num_tokens].bool()\n",
    "        attn_scores.masked_fill_(mask_bool, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = torch.matmul(attn_weights, value)\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch, num_tokens, -1)\n",
    "        return self.linear_projection(context_vec)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(config['embedding_dim'], config['embedding_dim'] * 4),\n",
    "            GELU(),\n",
    "            nn.Linear(config['embedding_dim'] * 4, config['embedding_dim'])\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layers(inputs)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mean = inputs.mean(dim=-1, keepdim=True)\n",
    "        var = inputs.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (inputs - mean) / torch.sqrt(var + self.eps)\n",
    "        return norm_x * self.scale + self.shift\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(\n",
    "            d_in=config['embedding_dim'], \n",
    "            d_out=config['embedding_dim'], \n",
    "            dropout=config['dropout'], \n",
    "            num_heads=config['n_heads'], \n",
    "            context_length=config['context_length']\n",
    "        )\n",
    "        self.norm1 = LayerNorm(config[\"embedding_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"embedding_dim\"])\n",
    "        self.ff = FeedForward(config)\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        add_connection = inputs\n",
    "        output = self.norm1(inputs)\n",
    "        output = self.attention(output)\n",
    "        output = self.dropout(output)\n",
    "        output = output + add_connection\n",
    "        \n",
    "        add_connection = output\n",
    "        output = self.norm2(output)\n",
    "        output = self.ff(output)\n",
    "        output = self.dropout(output)\n",
    "        output = output + add_connection\n",
    "        \n",
    "        return output\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'])\n",
    "        self.pos_embedding = nn.Embedding(config['context_length'], config['embedding_dim'])\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "        self.out_head = nn.Linear(config[\"embedding_dim\"], config[\"vocab_size\"], bias=False)\n",
    "        self.final_norm = LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len = inputs.shape\n",
    "        tok_embeds = self.token_embedding(inputs)\n",
    "        pos_embeds = self.pos_embedding(torch.arange(seq_len, device=inputs.device))\n",
    "        \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "\n",
    "    for step in range(max_new_tokens):  # Loop to generate tokens\n",
    "        #print(f\"Step {step + 1}: Generating new token...\\n\")\n",
    "\n",
    "        # 1. **Trim the input context** (model only supports a fixed length)\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        #print(f\"Context tokens (last {context_size} tokens):\\n{idx_cond}\\n\")\n",
    "        \n",
    "        # 2. **Pass the context into the model to get predictions**\n",
    "        with torch.no_grad():  # Disable gradients for efficiency\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        #print(f\"Logits shape (Batch x Tokens x Vocab Size): {logits.shape}\\n\")\n",
    "        \n",
    "        # 3. **Extract only the last token's logits**\n",
    "        logits = logits[:, -1, :]  # Shape becomes (batch, vocab_size)\n",
    "        #print(f\"Logits for the last predicted token:\\n{logits}\\n\")\n",
    "\n",
    "        # 4. **Convert logits to probabilities using softmax**\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        #print(f\"Probabilities after softmax:\\n{probas}\\n\")\n",
    "\n",
    "        # 5. **Choose the most probable next token**\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # Shape: (batch, 1)\n",
    "        #print(f\"Predicted next token index:\\n{idx_next}\\n\")\n",
    "\n",
    "        # 6. **Append the predicted token to the sequence**\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # Shape: (batch, n_tokens + 1)\n",
    "        #print(f\"Updated sequence:\\n{idx}\\n\")\n",
    "        #print(\"-\" * 50)  # Separator for readability\n",
    "\n",
    "    #print(\"\\nFinal generated sequence:\")\n",
    "    #print(idx)\n",
    "    return idx\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_embedding.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n",
      "Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n",
      "Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n",
      "Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n",
      "Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
      "Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n",
      "Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      "Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n",
      "Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
      "Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n",
      "Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (pos_embedding): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (final_norm): LayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = epochs_tensor = torch.linspace(0, num_epochs, len(train_losses)).tolist()\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you get to work.\n",
      "\n",
      "I turned--as the end one says half\n"
     ]
    }
   ],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=0\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you in spiteace,\" that robbed felt my picture ' by dep handsome, and\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (pos_embedding): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (final_norm): LayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (pos_embedding): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (final_norm): LayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = GPT(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model1.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model1,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=0.1\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (pos_embedding): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (W_querys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_keys): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_values): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (final_norm): LayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model2= GPT(GPT_CONFIG_124M)\n",
    "model2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model2.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.378, Val loss 6.476\n",
      "Ep 1 (Step 000005): Train loss 0.265, Val loss 6.511\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 2 (Step 000010): Train loss 0.199, Val loss 6.580\n",
      "Ep 2 (Step 000015): Train loss 0.190, Val loss 6.665\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the secret, and were amusing himself by holding\n",
      "Ep 3 (Step 000020): Train loss 0.136, Val loss 6.686\n",
      "Ep 3 (Step 000025): Train loss 0.114, Val loss 6.761\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, moved aside a _jardiniere_ full of\n",
      "Ep 4 (Step 000030): Train loss 0.118, Val loss 6.807\n",
      "Ep 4 (Step 000035): Train loss 0.096, Val loss 6.873\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model2, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
