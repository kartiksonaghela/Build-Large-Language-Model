{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Batch Shape: torch.Size([2, 6, 3]) (batch_size=2, seq_len=6, embedding_dim=3)\n",
      "MultiheadAttention(\n",
      "  (W_query): Linear(in_features=3, out_features=4, bias=False)\n",
      "  (W_key): Linear(in_features=3, out_features=4, bias=False)\n",
      "  (W_value): Linear(in_features=3, out_features=4, bias=False)\n",
      "  (linear_projection): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n",
      "\n",
      "[INFO] Input Shape: torch.Size([2, 6, 3]) (batch_size, seq_len, embedding_dim)\n",
      "\n",
      "[INFO] Q, K, V before reshaping:\n",
      "    Query Shape: torch.Size([2, 6, 4]) (should be batch_size, seq_len, d_out)\n",
      "    Key Shape: torch.Size([2, 6, 4])\n",
      "    Value Shape: torch.Size([2, 6, 4])\n",
      "\n",
      "[INFO] Q, K, V after reshaping into 2 heads:\n",
      "    Query Shape: torch.Size([2, 6, 2, 2]) (batch_size, seq_len, num_heads, head_dim)\n",
      "    Key Shape: torch.Size([2, 6, 2, 2])\n",
      "    Value Shape: torch.Size([2, 6, 2, 2])\n",
      "\n",
      "[INFO] Q, K, V after transposing:\n",
      "    Query Shape: torch.Size([2, 2, 6, 2]) (batch_size, num_heads, seq_len, head_dim)\n",
      "    Key Shape: torch.Size([2, 2, 6, 2])\n",
      "    Value Shape: torch.Size([2, 2, 6, 2])\n",
      "\n",
      "[INFO] Attention Scores Shape: torch.Size([2, 2, 6, 6]) (batch_size, num_heads, seq_len, seq_len)\n",
      "\n",
      "[INFO] Attention Weights Shape: torch.Size([2, 2, 6, 6])\n",
      "\n",
      "[INFO] Context Vector before reshaping: torch.Size([2, 2, 6, 2])\n",
      "\n",
      "[INFO] Context Vector after reshaping: torch.Size([2, 6, 4])\n",
      "\n",
      "[INFO] Context Vector after final projection: torch.Size([2, 6, 4]) (batch_size, seq_len, d_out)\n",
      "\n",
      "[INFO] Final Output:\n",
      "tensor([[[ 0.3442,  0.0560, -0.0691, -0.3827],\n",
      "         [ 0.3326, -0.0852,  0.0586, -0.4319],\n",
      "         [ 0.3284, -0.1331,  0.1036, -0.4476],\n",
      "         [ 0.3170, -0.1204,  0.1045, -0.4260],\n",
      "         [ 0.3179, -0.1273,  0.1083, -0.4311],\n",
      "         [ 0.3113, -0.1193,  0.1084, -0.4181]],\n",
      "\n",
      "        [[ 0.3442,  0.0560, -0.0691, -0.3827],\n",
      "         [ 0.3326, -0.0852,  0.0586, -0.4319],\n",
      "         [ 0.3284, -0.1331,  0.1036, -0.4476],\n",
      "         [ 0.3170, -0.1204,  0.1045, -0.4260],\n",
      "         [ 0.3179, -0.1273,  0.1083, -0.4311],\n",
      "         [ 0.3113, -0.1193,  0.1084, -0.4181]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.d_in = d_in\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear layers for Q, K, and V transformations\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.linear_projection = nn.Linear(d_out, d_out)  # Projection layer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask for autoregressive processing (ensuring only past tokens are attended)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch, num_tokens, dim = inputs.shape\n",
    "        print(f\"\\n[INFO] Input Shape: {inputs.shape} (batch_size, seq_len, embedding_dim)\")\n",
    "\n",
    "        # Compute Q, K, V matrices\n",
    "        query = self.W_query(inputs)  \n",
    "        key = self.W_key(inputs)      \n",
    "        value = self.W_value(inputs)  \n",
    "\n",
    "        print(f\"\\n[INFO] Q, K, V before reshaping:\")\n",
    "        print(f\"    Query Shape: {query.shape} (should be batch_size, seq_len, d_out)\")\n",
    "        print(f\"    Key Shape: {key.shape}\")\n",
    "        print(f\"    Value Shape: {value.shape}\")\n",
    "\n",
    "        # Reshape into multiple heads\n",
    "        query = query.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        key = key.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "        value = value.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        print(f\"\\n[INFO] Q, K, V after reshaping into {self.num_heads} heads:\")\n",
    "        print(f\"    Query Shape: {query.shape} (batch_size, seq_len, num_heads, head_dim)\")\n",
    "        print(f\"    Key Shape: {key.shape}\")\n",
    "        print(f\"    Value Shape: {value.shape}\")\n",
    "\n",
    "        # Transpose for correct matrix multiplication\n",
    "        query = query.transpose(1, 2)  \n",
    "        key = key.transpose(1, 2)      \n",
    "        value = value.transpose(1, 2)  \n",
    "\n",
    "        print(f\"\\n[INFO] Q, K, V after transposing:\")\n",
    "        print(f\"    Query Shape: {query.shape} (batch_size, num_heads, seq_len, head_dim)\")\n",
    "        print(f\"    Key Shape: {key.shape}\")\n",
    "        print(f\"    Value Shape: {value.shape}\")\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = query @ key.transpose(2, 3)  \n",
    "        print(f\"\\n[INFO] Attention Scores Shape: {attn_scores.shape} (batch_size, num_heads, seq_len, seq_len)\")\n",
    "\n",
    "        # Apply causal mask\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  \n",
    "        print(f\"\\n[INFO] Attention Weights Shape: {attn_weights.shape}\")\n",
    "\n",
    "        # Compute context vector\n",
    "        context_vec = attn_weights @ value  \n",
    "        print(f\"\\n[INFO] Context Vector before reshaping: {context_vec.shape}\")\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(batch, num_tokens, self.d_out)\n",
    "        print(f\"\\n[INFO] Context Vector after reshaping: {context_vec.shape}\")\n",
    "\n",
    "        # **Optional Projection Step**\n",
    "        # This linear projection ensures the output is in the expected feature space.\n",
    "        # It's useful for residual connections when stacking multiple attention layers.\n",
    "        context_vec = self.linear_projection(context_vec)\n",
    "        print(f\"\\n[INFO] Context Vector after final projection: {context_vec.shape} (batch_size, seq_len, d_out)\")\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "# Sample Input\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  \n",
    "    [0.55, 0.87, 0.66],  \n",
    "    [0.57, 0.85, 0.64],  \n",
    "    [0.22, 0.58, 0.33],  \n",
    "    [0.77, 0.25, 0.10],  \n",
    "    [0.05, 0.80, 0.55]   \n",
    "])\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)  \n",
    "print(f\"\\n[INFO] Batch Shape: {batch.shape} (batch_size=2, seq_len=6, embedding_dim=3)\")\n",
    "\n",
    "# Initialize Multihead Attention\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 4  \n",
    "multi_head = MultiheadAttention(d_in, d_out, context_length, dropout=0, num_heads=2)\n",
    "print(multi_head)\n",
    "\n",
    "# Run forward pass\n",
    "output = multi_head(batch)\n",
    "print(\"\\n[INFO] Final Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
