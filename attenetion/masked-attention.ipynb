{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([2, 6, 3])\n",
      "Batch data:\n",
      " tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      "Query matrix:\n",
      " tensor([[[ 0.1442, -0.4322, -0.1271],\n",
      "         [ 0.5539, -0.8205, -0.2508],\n",
      "         [ 0.5507, -0.8148, -0.2423],\n",
      "         [ 0.3345, -0.4630, -0.1617],\n",
      "         [ 0.3364, -0.4829, -0.0206],\n",
      "         [ 0.3846, -0.5472, -0.2521]],\n",
      "\n",
      "        [[ 0.1442, -0.4322, -0.1271],\n",
      "         [ 0.5539, -0.8205, -0.2508],\n",
      "         [ 0.5507, -0.8148, -0.2423],\n",
      "         [ 0.3345, -0.4630, -0.1617],\n",
      "         [ 0.3364, -0.4829, -0.0206],\n",
      "         [ 0.3846, -0.5472, -0.2521]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Key matrix:\n",
      " tensor([[[ 0.3382,  0.0819,  0.1610],\n",
      "         [ 0.4117,  0.1051,  0.0470],\n",
      "         [ 0.3877,  0.1074,  0.0307],\n",
      "         [ 0.2732,  0.0466,  0.0502],\n",
      "         [-0.1532,  0.1177, -0.2703],\n",
      "         [ 0.5184,  0.0291,  0.2083]],\n",
      "\n",
      "        [[ 0.3382,  0.0819,  0.1610],\n",
      "         [ 0.4117,  0.1051,  0.0470],\n",
      "         [ 0.3877,  0.1074,  0.0307],\n",
      "         [ 0.2732,  0.0466,  0.0502],\n",
      "         [-0.1532,  0.1177, -0.2703],\n",
      "         [ 0.5184,  0.0291,  0.2083]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Value matrix:\n",
      " tensor([[[-0.0728, -0.2630, -0.1198],\n",
      "         [-0.1468, -0.0461,  0.0780],\n",
      "         [-0.1366, -0.0359,  0.0608],\n",
      "         [-0.1097, -0.0164,  0.1172],\n",
      "         [ 0.0852,  0.1589, -0.2664],\n",
      "         [-0.2123, -0.1183,  0.2831]],\n",
      "\n",
      "        [[-0.0728, -0.2630, -0.1198],\n",
      "         [-0.1468, -0.0461,  0.0780],\n",
      "         [-0.1366, -0.0359,  0.0608],\n",
      "         [-0.1097, -0.0164,  0.1172],\n",
      "         [ 0.0852,  0.1589, -0.2664],\n",
      "         [-0.2123, -0.1183,  0.2831]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Raw Attention Scores:\n",
      " tensor([[[-0.0071,  0.0080,  0.0056,  0.0129, -0.0386,  0.0357],\n",
      "         [ 0.0797,  0.1300,  0.1190,  0.1006, -0.1136,  0.2110],\n",
      "         [ 0.0805,  0.1297,  0.1186,  0.1003, -0.1147,  0.2112],\n",
      "         [ 0.0492,  0.0815,  0.0750,  0.0617, -0.0620,  0.1262],\n",
      "         [ 0.0709,  0.0867,  0.0779,  0.0684, -0.1028,  0.1560],\n",
      "         [ 0.0447,  0.0890,  0.0826,  0.0670, -0.0551,  0.1309]],\n",
      "\n",
      "        [[-0.0071,  0.0080,  0.0056,  0.0129, -0.0386,  0.0357],\n",
      "         [ 0.0797,  0.1300,  0.1190,  0.1006, -0.1136,  0.2110],\n",
      "         [ 0.0805,  0.1297,  0.1186,  0.1003, -0.1147,  0.2112],\n",
      "         [ 0.0492,  0.0815,  0.0750,  0.0617, -0.0620,  0.1262],\n",
      "         [ 0.0709,  0.0867,  0.0779,  0.0684, -0.1028,  0.1560],\n",
      "         [ 0.0447,  0.0890,  0.0826,  0.0670, -0.0551,  0.1309]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Masked Attention Scores:\n",
      " tensor([[[-0.0071,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.0797,  0.1300,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.0805,  0.1297,  0.1186,    -inf,    -inf,    -inf],\n",
      "         [ 0.0492,  0.0815,  0.0750,  0.0617,    -inf,    -inf],\n",
      "         [ 0.0709,  0.0867,  0.0779,  0.0684, -0.1028,    -inf],\n",
      "         [ 0.0447,  0.0890,  0.0826,  0.0670, -0.0551,  0.1309]],\n",
      "\n",
      "        [[-0.0071,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.0797,  0.1300,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.0805,  0.1297,  0.1186,    -inf,    -inf,    -inf],\n",
      "         [ 0.0492,  0.0815,  0.0750,  0.0617,    -inf,    -inf],\n",
      "         [ 0.0709,  0.0867,  0.0779,  0.0684, -0.1028,    -inf],\n",
      "         [ 0.0447,  0.0890,  0.0826,  0.0670, -0.0551,  0.1309]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "\n",
      "Attention Weights (After Softmax):\n",
      " tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4927, 0.5073, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3278, 0.3372, 0.3350, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2475, 0.2521, 0.2512, 0.2493, 0.0000, 0.0000],\n",
      "         [0.2034, 0.2053, 0.2042, 0.2031, 0.1840, 0.0000],\n",
      "         [0.1651, 0.1694, 0.1688, 0.1673, 0.1559, 0.1736]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4927, 0.5073, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3278, 0.3372, 0.3350, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2475, 0.2521, 0.2512, 0.2493, 0.0000, 0.0000],\n",
      "         [0.2034, 0.2053, 0.2042, 0.2031, 0.1840, 0.0000],\n",
      "         [0.1651, 0.1694, 0.1688, 0.1673, 0.1559, 0.1736]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Context Vector (Final Output):\n",
      " tensor([[[-0.1456, -0.5259, -0.2395],\n",
      "         [-0.1489, -0.0468,  0.0791],\n",
      "         [-0.0915, -0.0240,  0.0407],\n",
      "         [-0.1101, -0.1534, -0.0200],\n",
      "         [-0.0296, -0.1070, -0.0487],\n",
      "         [-0.0497, -0.0156,  0.0264]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000],\n",
      "         [-0.1489, -0.0468,  0.0791],\n",
      "         [-0.1905, -0.0551,  0.0933],\n",
      "         [-0.1647, -0.1616,  0.0385],\n",
      "         [-0.0847,  0.0249, -0.0412],\n",
      "         [-0.0712, -0.0784, -0.0244]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1456, -0.5259, -0.2395],\n",
       "         [-0.1489, -0.0468,  0.0791],\n",
       "         [-0.0915, -0.0240,  0.0407],\n",
       "         [-0.1101, -0.1534, -0.0200],\n",
       "         [-0.0296, -0.1070, -0.0487],\n",
       "         [-0.0497, -0.0156,  0.0264]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.1489, -0.0468,  0.0791],\n",
       "         [-0.1905, -0.0551,  0.0933],\n",
       "         [-0.1647, -0.1616,  0.0385],\n",
       "         [-0.0847,  0.0249, -0.0412],\n",
       "         [-0.0712, -0.0784, -0.0244]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define input\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "    [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "    [0.05, 0.80, 0.55]   # step     (x^6)\n",
    "])\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)  # Create a batch of size (2, 6, 3)\n",
    "print(\"Batch shape:\", batch.shape)\n",
    "print(\"Batch data:\\n\", batch)\n",
    "\n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(3, 3, bias=False)\n",
    "        self.W_key = nn.Linear(3, 3, bias=False)\n",
    "        self.W_value = nn.Linear(3, 3, bias=False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(6, 6), diagonal=1))  # Upper triangular mask\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        b, context_length, dim = inputs.shape\n",
    "        \n",
    "        key = self.W_key(inputs)\n",
    "        value = self.W_value(inputs)\n",
    "        query = self.W_query(inputs)\n",
    "\n",
    "        print(\"\\nQuery matrix:\\n\", query)\n",
    "        print(\"\\nKey matrix:\\n\", key)\n",
    "        print(\"\\nValue matrix:\\n\", value)\n",
    "\n",
    "        # Compute raw attention scores\n",
    "        attention_scores = query @ key.transpose(1, 2)\n",
    "        print(\"\\nRaw Attention Scores:\\n\", attention_scores)\n",
    "\n",
    "        # Apply mask (causal masking)\n",
    "        masked_attention_scores = attention_scores.clone()\n",
    "        masked_attention_scores.masked_fill_(self.mask.bool()[:context_length, :context_length], -torch.inf)\n",
    "        print(\"\\nMasked Attention Scores:\\n\", masked_attention_scores)\n",
    "\n",
    "        # Compute softmax to get attention weights\n",
    "        attn_weights = torch.softmax(masked_attention_scores / (key.shape[-1] ** 0.5), dim=-1)\n",
    "        print(\"\\nAttention Weights (After Softmax):\\n\", attn_weights)\n",
    "\n",
    "        # Apply dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Compute final attention output\n",
    "        context_vector = attn_weights @ value\n",
    "        print(\"\\nContext Vector (Final Output):\\n\", context_vector)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "# Initialize and run masked attention\n",
    "masked_attention = MaskedAttention()\n",
    "torch.manual_seed(123)\n",
    "masked_attention(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
